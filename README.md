## NN

### **Homework 1:** Saliency maps for Explanaibility
References:

&ensp;&thinsp;&ensp; [1] Simonyan, K., Vedaldi, A. and Zisserman, A., 2013.\
&ensp;&thinsp;&ensp;&thinsp;&ensp; [Deep inside convolutional networks: Visualising image classification models and saliency maps](https://arxiv.org/abs/1312.6034). \
&ensp;&thinsp;&ensp;&thinsp;&ensp; arXiv preprint arXiv:1312.6034.

&ensp;&thinsp;&ensp; [2] Smilkov, D., Thorat, N., Kim, B., Vi√©gas, F. and Wattenberg, M., 2017. \
&ensp;&thinsp;&ensp;&thinsp;&ensp; [SmoothGrad: removing noise by adding noise](https://arxiv.org/abs/1706.03825). \
&ensp;&thinsp;&ensp;&thinsp;&ensp; arXiv preprint arXiv:1706.03825.

&ensp;&thinsp;&ensp;[3] Rudin, C., 2019. 1 \
&ensp;&thinsp;&ensp;&thinsp;&ensp; [Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://www.nature.com/articles/s42256-019-0048-x).\
&ensp;&thinsp;&ensp;&thinsp;&ensp; Nature Machine Intelligence, 1(5), pp. 206-215.
