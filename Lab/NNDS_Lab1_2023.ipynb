{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks for Data Science Applications (2023-2024)\n",
        "### Lab session 1: Logistic regression in TensorFlow\n",
        "\n",
        "Teaching assistants: [Jary Pomponi](https://scholar.google.com/citations?user=Zha7UeoAAAAJ&hl=it), [Francesco Verdini](https://phd.uniroma1.it/web/FRANCESCO-VERDINI_nP1765820_EN.aspx)"
      ],
      "metadata": {
        "id": "AR03hjh3GWog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1: Setup of the notebook"
      ],
      "metadata": {
        "id": "pwTwlFISGmlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# At the moment of writing, the latest TF version is 2.14, the one installed\n",
        "# on Colab is 2.13. If you want the latest one, uncomment this line.\n",
        "# %pip install tensorflow --upgrade --quiet"
      ],
      "metadata": {
        "id": "8gHrnHooGxhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1aoyc2wOAiz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check available devices. By default, Colab should only have a single CPU device.\n",
        "tf.config.list_logical_devices()"
      ],
      "metadata": {
        "id": "KbFyQadMSvTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2: Manipulating tensors"
      ],
      "metadata": {
        "id": "PDQOhgLmG6dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a simple tensor whose entries are drawn from a normal distribution.\n",
        "x = tf.random.normal((3, 2))"
      ],
      "metadata": {
        "id": "HPQCyp_sG8th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectors can be scaled (this will be equivalent to tf.random.normal((3, 2), mean=10, stddev=0.1)).\n",
        "x = tf.random.normal((3, 2)) * 0.1 + 10"
      ],
      "metadata": {
        "id": "Od8nUki9S5Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "id": "xMG9jhf4TMyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# By default, a tensor is represented by its shape and type.\n",
        "print(x.shape)\n",
        "print(x.dtype)"
      ],
      "metadata": {
        "id": "WPRkHLr2HDGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract a NumPy representation\n",
        "x.numpy()"
      ],
      "metadata": {
        "id": "TAtLttTETzRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexing is based on the NumPy semantics.\n",
        "# Check here for a more complete guide: https://www.tensorflow.org/guide/tensor\n",
        "x[0:2]"
      ],
      "metadata": {
        "id": "zFEXFUD1T8AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape can modify the dimensions and also add/remove singleton dimensions.\n",
        "tf.reshape(x, (1, 6, 1, 1))"
      ],
      "metadata": {
        "id": "LsqVf4bSUB51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This example shows how two tensors can be multiplied even if the dimensions are different by using broadcasting.\n",
        "# Also, it shows how new axes can be added using None.\n",
        "x[:, None] * x[None]"
      ],
      "metadata": {
        "id": "76X6RPsOUXLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 3: Automatic differentiation"
      ],
      "metadata": {
        "id": "s5rCbdJEHX-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z = tf.random.normal((3, 3))\n",
        "X = tf.random.normal((3, 2))\n",
        "\n",
        "# GradientTape stores the intermediate computations to allow for gradient computation.\n",
        "# In order to store a computation, it must originate from a tensor that is 'watched',\n",
        "# or encapsulated inside a tf.Variable (see below).\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(X)\n",
        "    tape.watch(Z)\n",
        "    y = tf.reduce_mean(tf.math.cos(X) @ tf.transpose(X) + Z)"
      ],
      "metadata": {
        "id": "ZoSW-H18Vn9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: The operator @ performs a matrix multiplication, which calls tf.matmul() under the hood (https://www.tensorflow.org/api_docs/python/tf/linalg/matmul).\n",
        "tf.math.cos(X) @ tf.transpose(X)"
      ],
      "metadata": {
        "id": "LGmIupmMV2TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradients of y with respect to X and Z.\n",
        "tape.gradient(y, [X, Z])"
      ],
      "metadata": {
        "id": "qlEfSGl8HmTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executing the gradient twice gives an error, because resources\n",
        "# are immediately freed as soon as the gradient is computed: try it by\n",
        "# uncommenting this line.\n",
        "# tape.gradient(y, [X, Z])"
      ],
      "metadata": {
        "id": "Zpf1i5fwHrbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison between the same operation performed with and without using a Context Manager.\n",
        "'''\n",
        "With\n",
        "with open(...) as f:\n",
        "  f.read()\n",
        "\n",
        "Without\n",
        "f = open(...)\n",
        "f.read()\n",
        "f.close()\n",
        "'''"
      ],
      "metadata": {
        "id": "a3_ExATkXCbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just an example of how the same thing can be done without a context manager, even if the GradientTape class discourages it.\n",
        "tape = tf.GradientTape()\n",
        "tape._push_tape()\n",
        "tape.watch(X)\n",
        "tape.watch(Z)\n",
        "y = tf.reduce_mean(tf.math.cos(X) @ tf.transpose(X) + Z)\n",
        "tape.gradient(y, X)"
      ],
      "metadata": {
        "id": "hrBsdrywXJFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 4: Logistic regression"
      ],
      "metadata": {
        "id": "2cbKwa19HyHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "lc65x7_FH4dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See here to learn more about the dataset: https://www.tensorflow.org/datasets/catalog/penguins\n",
        "train_data = tfds.load('penguins', as_supervised=True, split='train[0:80%]')\n",
        "test_data = tfds.load('penguins', as_supervised=True, split='train[80%:]')"
      ],
      "metadata": {
        "id": "xg53Vk6PZKrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets in TF are built as iterators over the single elements (we will see more about\n",
        "# tf.data in the next lecture).\n",
        "train_data"
      ],
      "metadata": {
        "id": "tJHFvnzmH7ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code extracts the two tensors from the iterator.\n",
        "Xtrain, ytrain = train_data.batch(1000).get_single_element()"
      ],
      "metadata": {
        "id": "0f2rJh9Ya1-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each row is an example, each column a feature of the input.\n",
        "Xtrain.shape"
      ],
      "metadata": {
        "id": "_U-vnLErIAKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain"
      ],
      "metadata": {
        "id": "RO6l6wtKa4UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each element is the true class of the corresponding row in X.\n",
        "print(ytrain.shape)\n",
        "print(ytrain[0:10])"
      ],
      "metadata": {
        "id": "XrM_dYP4ICYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytrain"
      ],
      "metadata": {
        "id": "9XLFmNZZbciw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We one-hot encode the y tensor, which turns it into a (n, 3) tensor.\n",
        "ytrain = tf.one_hot(tf.cast(ytrain, tf.int32), 3)\n",
        "print(ytrain.shape)\n",
        "print(ytrain[0:5])"
      ],
      "metadata": {
        "id": "Qd_TCZxdbSUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We do the same for the test part of the dataset.\n",
        "Xtest, ytest = test_data.batch(1000).get_single_element()\n",
        "ytest = tf.one_hot(ytest, 3)"
      ],
      "metadata": {
        "id": "4w-KQZRrcccE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init():\n",
        "  # Initialize the parameters of the logistic regression model.\n",
        "  # Any tensor wrapped inside tf.Variable is automatically watched inside the GradientTape.\n",
        "  W = tf.Variable(tf.random.normal((4, 3)))\n",
        "  b = tf.Variable(tf.random.normal((3,)))\n",
        "  return W, b"
      ],
      "metadata": {
        "id": "QuEQ88t0ce6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logreg(X, W, b):\n",
        "  # Logistic regression model (note how the softmax is applied row-wise).\n",
        "  return tf.nn.softmax(X @ W + b , 1)"
      ],
      "metadata": {
        "id": "dBYALrWpdLQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W, b = init()\n",
        "ypred = logreg(Xtrain, W, b)\n",
        "print(ypred.shape)"
      ],
      "metadata": {
        "id": "_0kBRdo7dReq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(ytrue, ypred):\n",
        "  \"\"\" Compute the average cross-entropy over the elements.\n",
        "  Inputs:\n",
        "  - ytrue (n, 3): one-hot encoded tensor of the correct labels.\n",
        "  - ypred (n, 3): output of the logistic regression models (after the softmax).\n",
        "\n",
        "  Returns a scalar which is the average cross-entropy.\n",
        "  \"\"\"\n",
        "  return -tf.reduce_mean(ytrue * tf.math.log(ypred))"
      ],
      "metadata": {
        "id": "_00X9BG9eCIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy(ytrain, ypred)"
      ],
      "metadata": {
        "id": "bfDaqQZDebS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(ytrue, ypred):\n",
        "  \"\"\" Compute the average accuracy over the elements. Input parameters are\n",
        "      the same as for the cross-entropy loss function.\n",
        "  \"\"\"\n",
        "  # Note the casting operation, since we cannot take the average of a boolean vector.\n",
        "  return tf.reduce_mean(\n",
        "        tf.cast(tf.argmax(ytrue, 1) == tf.argmax(ypred, 1), tf.float32)\n",
        "      )"
      ],
      "metadata": {
        "id": "Jo_GjDlpepy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(ytrain, ypred)"
      ],
      "metadata": {
        "id": "1katRzpwe9pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "W, b = init()\n",
        "\n",
        "for i in range(5000):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # Get the predictions of the model\n",
        "        ypred = logreg(Xtrain, W, b)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = cross_entropy(ytrain, ypred)\n",
        "\n",
        "    # Compute the gradients\n",
        "    gradients = tape.gradient(loss, [W, b])\n",
        "\n",
        "    # Apply the gradients\n",
        "    W.assign_sub(0.1*gradients[0])\n",
        "    b.assign_sub(0.1*gradients[1])\n",
        "\n",
        "    # This is an incorrect version, since the result is not a tf.Variable anymore.\n",
        "    # W = W - 0.01*gradients[0]\n",
        "    # b = b - 0.01*gradients[1]\n",
        "\n",
        "    # Track interesting quantities\n",
        "    losses.append(loss.numpy())\n",
        "    accuracies.append(accuracy(ytrain, ypred).numpy())\n",
        "\n",
        "plt.plot(losses, label='loss')\n",
        "plt.plot(accuracies, label='accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "LRkOT6MjfdOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy(ytest, logreg(Xtest, W, b)))"
      ],
      "metadata": {
        "id": "ZhKF4vk9hTUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W.numpy().mean(-1)"
      ],
      "metadata": {
        "id": "rkq-aznyiVTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 5: Logistic regression, reloaded\n",
        "This is similar to before, but we will use a few high-level components from TensorFlow instead of reimplementing them ourselves."
      ],
      "metadata": {
        "id": "BNgM45YTW6Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Most high-level components are inside the tf.keras module.\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "05alm_tsi2qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layers are defined by how you initialize their variables,\n",
        "# and how they process data, similar to init / logreg above.\n",
        "# Learn more here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer\n",
        "model = keras.layers.Dense(3)"
      ],
      "metadata": {
        "id": "0gZeCAY0i3cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip this if you have not read L4 (Fully-connected models).\n",
        "# You can easily replace the logistic-regression model with a fully-connected\n",
        "# model by using a Sequential Model from TensorFlow to \"stitch\"\n",
        "# together two different fully-connected blocks.\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
        "# model = keras.Sequential([\n",
        "#    keras.layers.Dense(50, activation=keras.activations.relu),\n",
        "#    keras.layers.Dense(3)\n",
        "# ])"
      ],
      "metadata": {
        "id": "X6jyn14ZYhJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is an alternative syntax (the model is equivalent\n",
        "# to the one above).\n",
        "# model = keras.Sequential()\n",
        "# model.add(keras.layers.Dense(50, activation=keras.activations.relu))\n",
        "# model.add(keras.layers.Dense(3))"
      ],
      "metadata": {
        "id": "Wl-0cMPRYjih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The variables are lazily created only the first time the model is called (see below).\n",
        "# Note: this will raise an error for the tf.keras.Sequential model.\n",
        "model.variables"
      ],
      "metadata": {
        "id": "qV225N7BXNqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(Xtrain).shape"
      ],
      "metadata": {
        "id": "4sxOMhTWjIM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that by default the biases are initialized to zero.\n",
        "[v.shape for v in model.variables]"
      ],
      "metadata": {
        "id": "3S4BP9YxXRnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We move back to an index-based representation.\n",
        "ytrain = tf.argmax(ytrain, 1)\n",
        "ytest = tf.argmax(ytest, 1)"
      ],
      "metadata": {
        "id": "JusiJ7XgjP0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = model(Xtrain)"
      ],
      "metadata": {
        "id": "5qJH3qMrXc7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functional variant of the cross-entropy (see the slides for more information\n",
        "# about the different variants).\n",
        "tf.reduce_mean(\n",
        "    keras.losses.sparse_categorical_crossentropy(ytrain, ypred, from_logits=True)\n",
        ")"
      ],
      "metadata": {
        "id": "07zs03YWj3la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Object-oriented variant.\n",
        "keras.losses.SparseCategoricalCrossentropy(from_logits=True)(ytrain, ypred)"
      ],
      "metadata": {
        "id": "WOxNe8ArkJgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.1)"
      ],
      "metadata": {
        "id": "2o7GJeEYKZZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "for i in range(5000):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        ypred = model(Xtrain)\n",
        "        loss = cross_entropy(ytrain, ypred)\n",
        "\n",
        "    # --> First difference: we differentiate w.r.t. all variables.\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # --> We use the optimizer to apply the gradients.\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    losses.append(loss.numpy())\n",
        "    accuracies.append(accuracy(ytrain, ypred).numpy())\n",
        "\n",
        "plt.plot(losses, label='loss')\n",
        "plt.plot(accuracies, label='accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "h93DMhZpkmQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercises\n",
        "\n",
        "1. Modify part 4 to use the index-based version of y instead of the one-hot encoded version (hint: you need to suitably modify the `cross_entropy` and `accuracy` methods).\n",
        "\n",
        "2. Optimizers have a `minimize()` function, allowing to combine the computation of the gradient with the gradient descent step:\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#usage. Rewrite the train loop in step 5 to use `minimize` instead of `apply_gradients`.\n",
        "\n",
        "3. Momentum is a simple technique to improve the convergence speed of gradient descent. The key idea is to update each variable using a weighted average of the current gradient, and the gradient at the previous iteration (see Section 12.6 in the book). The weighting parameter is called the momentum weight. Implement momentum in the codelab, using a weight of 0.5, both in part 4 (manually) and in part 5 (using the `momentum` parameter of `SGD`)."
      ],
      "metadata": {
        "id": "WOpDYDGpSr8o"
      }
    }
  ]
}